{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b64a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "# from pettingzoo.classic import chess_v6\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import brl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9437d497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applaying Reinforce_n=8_SGD_lr=2e-14_momentum=0.0001_nesterov=True on connect_four_v3\n"
     ]
    }
   ],
   "source": [
    "algo = \"Reinforce\"\n",
    "\n",
    "environment = connect_four_v3\n",
    "environment_name = environment.__name__.split(\".\")[-1]\n",
    "\n",
    "if environment_name == \"tictactoe_v3\":\n",
    "    nb_actions = 9\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(18, 32),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(32, nb_actions),\n",
    "          torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.SGD\n",
    "    optimizer_args = {\n",
    "        \"lr\": 3e-3,\n",
    "        \"momentum\": 0.01,\n",
    "        \"nesterov\": True\n",
    "    }\n",
    "\n",
    "    nb_episodes_before_opti = 4\n",
    "    nb_epoch = 40_000\n",
    "\n",
    "elif environment_name == \"connect_four_v3\":\n",
    "    nb_actions = 7\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(84, 512),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(512, 256),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(256, 128),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(128, nb_actions),\n",
    "          # torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.SGD\n",
    "    optimizer_args = {\n",
    "        \"lr\": 2e-14,\n",
    "        \"momentum\": 0.0001,\n",
    "        \"nesterov\": True\n",
    "    }\n",
    "\n",
    "    nb_episodes_before_opti = 8\n",
    "    nb_epoch = 1_000_000\n",
    "\n",
    "elif environment_name == \"chess_v6\":\n",
    "    nb_actions = 7\n",
    "    learning_rate = 1e-4\n",
    "    nb_episodes_before_opti = 32\n",
    "\n",
    "    nb_epoch = 400_000\n",
    "\n",
    "exp_name_build_name = algo + \"_n=\" + str(nb_episodes_before_opti) + \"_\" + str(optimizer.__name__) + \"\".join([\"_\" + args + \"=\" + str(optimizer_args[args]) for args in optimizer_args.keys()])\n",
    "\n",
    "print(\"Applaying\", exp_name_build_name, \"on\", environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c840afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class RandomPolicy():\n",
    "    def __init__(self, action_size):\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def act(self, observation, mask=None):\n",
    "        actions = np.arange(self.action_size)\n",
    "        actions = actions[mask.astype(bool)]\n",
    "        return np.random.choice(actions)\n",
    "\n",
    "class RandomAgent():\n",
    "\n",
    "    def __init__(self, policy):\n",
    "        self.loss = 0\n",
    "        self.policy = policy\n",
    "\n",
    "    def observe(self, observation, action, reward, next_state, terminated):\n",
    "        pass\n",
    "\n",
    "    def optimize(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0145e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyPolicy(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(MyPolicy, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float().flatten(start_dim=-3)\n",
    "        return self.model(x)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def act(self, observation, mask):\n",
    "        observation = torch.from_numpy(observation).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        action_values = self.forward(observation)\n",
    "        probabilities = (torch.nn.functional.softmax(action_values, dim=0) * mask) + (1e-12 * mask)\n",
    "        action = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        return action\n",
    "\n",
    "\n",
    "def _init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851808c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "experiment_name = environment_name + \"/\" + exp_name_build_name\n",
    "log_dir = os.path.join(\"../../tensorboard\", experiment_name)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "def run_env(env, agents):\n",
    "    env.reset()\n",
    "\n",
    "    last_actions = {agent: None for agent in agents.keys()}\n",
    "    last_observation = {agent: None for agent in agents.keys()}\n",
    "\n",
    "    last_reward = {agent: None for agent in agents.keys()}\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        obs = observation[\"observation\"]\n",
    "        mask = observation[\"action_mask\"]\n",
    "\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "            last_reward[agent] = reward\n",
    "            agents[agent].observe(last_observation[agent], last_actions[agent], reward, obs, True)\n",
    "        else:\n",
    "            if last_observation[agent] is not None:\n",
    "                agents[agent].observe(last_observation[agent], last_actions[agent], reward, obs, False)\n",
    "\n",
    "            # this is where you would insert your policy\n",
    "            action = agents[agent].policy.act(obs, mask)\n",
    "\n",
    "            last_observation[agent] = obs\n",
    "            last_actions[agent] = action\n",
    "\n",
    "        env.step(action)\n",
    "    \n",
    "    return last_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5d9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [0] * 1000\n",
    "\n",
    "def train(env, agents, n):\n",
    "    agent_to_log = env.agents[0]\n",
    "\n",
    "    for epoch in range(n + 1):\n",
    "        reward = run_env(env, agents)\n",
    "        rewards[epoch % len(rewards)] = reward[\"player_1\"]\n",
    "\n",
    "        if epoch % len(rewards) == 0:\n",
    "            writer.add_scalar(\"train/mean_reward\", sum(rewards) / len(rewards), epoch)\n",
    "            writer.add_scalar(\"train/loss\", agents[agent_to_log].loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41edcc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['player_0', 'player_1']\n",
      "(6, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "env = environment.env(render_mode=\"rgb\")\n",
    "env.reset()\n",
    "\n",
    "state_shape = env.observation_space(\"player_1\")[\"observation\"].shape\n",
    "\n",
    "print(env.agents)\n",
    "print(env.observation_space(\"player_1\")[\"observation\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2557d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = MyPolicy(model=model)\n",
    "policy.apply(_init_weights)\n",
    "\n",
    "reinforce_agent = brl.reinforce.Reinforce(policy=policy, optimizer=optimizer, optimizer_parameters=optimizer_args, nb_episodes=nb_episodes_before_opti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1becdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = RandomPolicy(action_size=nb_actions)\n",
    "random_agent = RandomAgent(policy=random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c6dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {\n",
    "    env.agents[0]: reinforce_agent,\n",
    "    env.agents[1]: random_agent\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "249fcd84",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnb_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env, agents, n)\u001b[39m\n\u001b[32m      4\u001b[39m agent_to_log = env.agents[\u001b[32m0\u001b[39m]\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     reward = \u001b[43mrun_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     rewards[epoch % \u001b[38;5;28mlen\u001b[39m(rewards)] = reward[\u001b[33m\"\u001b[39m\u001b[33mplayer_1\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[38;5;28mlen\u001b[39m(rewards) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mrun_env\u001b[39m\u001b[34m(env, agents)\u001b[39m\n\u001b[32m     23\u001b[39m     action = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     24\u001b[39m     last_reward[agent] = reward\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_observation\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m last_observation[agent] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/bright-rl/src/brl/reinforce/reinforce.py:23\u001b[39m, in \u001b[36mReinforce.observe\u001b[39m\u001b[34m(self, observation, action, reward, next_state, terminated)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.episodes_in_memory += terminated\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.episodes_in_memory == \u001b[38;5;28mself\u001b[39m.nb_episodes:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mself\u001b[39m.memory = []\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mself\u001b[39m.episodes_in_memory = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/bright-rl/src/brl/reinforce/reinforce.py:48\u001b[39m, in \u001b[36mReinforce.optimize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_value_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/bright-rl/.venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:30\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/bright-rl/.venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:179\u001b[39m, in \u001b[36mclip_grad_value_\u001b[39m\u001b[34m(parameters, clip_value, foreach)\u001b[39m\n\u001b[32m    174\u001b[39m grouped_grads = _group_tensors_by_device_and_dtype([grads])\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([grads], _) \u001b[38;5;129;01min\u001b[39;00m grouped_grads.items():  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    178\u001b[39m         foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[43m_has_foreach_support\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     ) \u001b[38;5;129;01mor\u001b[39;00m (foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)):\n\u001b[32m    181\u001b[39m         torch._foreach_clamp_min_(cast(List[Tensor], grads), -clip_value)\n\u001b[32m    182\u001b[39m         torch._foreach_clamp_max_(cast(List[Tensor], grads), clip_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/bright-rl/.venv/lib/python3.11/site-packages/torch/utils/_foreach_utils.py:44\u001b[39m, in \u001b[36m_has_foreach_support\u001b[39m\u001b[34m(tensors, device)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_has_foreach_support\u001b[39m(tensors: List[Tensor], device: torch.device) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_device_has_foreach_support\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;129;01min\u001b[39;00m _foreach_supported_types \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/bright-rl/.venv/lib/python3.11/site-packages/torch/utils/_foreach_utils.py:40\u001b[39m, in \u001b[36m_device_has_foreach_support\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_device_has_foreach_support\u001b[39m(device: torch.device) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m device.type \u001b[38;5;129;01min\u001b[39;00m (\u001b[43m_get_foreach_kernels_supported_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m + [\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/bright-rl/.venv/lib/python3.11/site-packages/torch/utils/_foreach_utils.py:8\u001b[39m, in \u001b[36m_get_foreach_kernels_supported_devices\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograd\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgrad_mode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m no_grad\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeAlias\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_foreach_kernels_supported_devices\u001b[39m() -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return the device type list that supports foreach kernels.\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m, torch._C._get_privateuse1_backend_name()]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(env, agents, n=nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a3232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
